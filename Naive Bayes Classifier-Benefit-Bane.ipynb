{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Simplicity of Naive Bayes Classifiers: Benefit and Bane\n",
    "\n",
    "\n",
    "The goal of this notebook is to understand the **benefit and bane** of the Naive Bayes (NB) algorithm.\n",
    "\n",
    "More specifically, we will **<font color=red> investigate the limitation</font>** of the NB classifier.\n",
    "\n",
    "First, let's briefly discuss the benefit of the NB classifier.\n",
    "\n",
    "\n",
    "## Benefit of \"Simplicity\"\n",
    "\n",
    "The Naive Bayes algorithm is fueled by two \"simplicity\".\n",
    "    - Simple inductive bias: conditional independence\n",
    "    - Simple computation: statistical estimation of the prior and likelihood\n",
    "\n",
    "\n",
    "In the NB classifier, we make a strong assumption: **given the class, the features are conditionaly independent**. The model is called \"naive\" because we do not expect the features to be independent, even conditional on the class label.\n",
    "\n",
    "However, for solving text classification problems we have seen that although the naive Bayes assumption is an extremely strong assumption, the performance of the NB classifier was surprisingly good.\n",
    "\n",
    "The reason NB classifier works well in text classification problems is that the conditional independence is a reasonalble assumption in the context of text classification. \n",
    "\n",
    "As an example, consider classifying emails into spam and ham. Words like \"lottery\", \"prize\" and \"win\" are all likely indicators that the email might be spam. On the other hand words like \"Bayes\", \"Learning\", \"Classifier\" are good indicators of a ham email. Thus, we could model the **probability of occurrence for each of these words independently, given the respective class** and then use it to score the likelihood of a text.\n",
    "\n",
    "\n",
    "\n",
    "## Bane of \"Simplicity\"\n",
    "\n",
    "However, the conditional independence assumption might not hold in some domains. For example, in image classification problems if we use pixel values to model the feature vectors, then the NB assumption fails. The reason is that the feature values (pixels) are not conditionally independent. The **pixels are not independently generated, depending only on the label**. If we look at a black pixel at one of the corners, the other pixels around is is likely to be black.\n",
    "\n",
    "Thus, the NB assumption does not hold for the feature values (pixels) in an image classification problem.\n",
    "\n",
    "Because of its simplicity, Naive Bayes was a very popular choice for building spam filters during 80s and 90s. However, for classifying images it is not as effective. The main reason, as we identified, is the inadequacy of its simplicity! The inductive bias (conditional independence assumption) simply doesn't hold in many domains (e.g., image classification).\n",
    "\n",
    "It is due to this incorrect assumption about the \"world\", NB classifiers lost its popularity. Advanced models such as Artificial Neural Network was invented for image classification. \n",
    "\n",
    "\n",
    "In this notebook, we will use NB classifier to solve an image classification problem. We will compare the performance of the NB classifier with the K-Nearest Neighbor (KNN) memory-based model that we learned previously.\n",
    "\n",
    "\n",
    "## Datset\n",
    "\n",
    "We will use the MNIST dataset. It is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. \n",
    "\n",
    "Each image is labeled with the digit it represents. Each image is represented by 28 x 28 gray scale. There are 784 pixel values. Each value represents a pixel’s intensity, from 0 (white) to 255 (black). \n",
    "\n",
    "\n",
    "## NB Models for Investigation\n",
    "\n",
    "We will model a feature vector (an image) as a 784 dimensional vector. Each dimension represents an individual pixel value. Since the pixels represent light intensity in the range of 0 ~ 255, we have two choices to model the feature values.\n",
    "\n",
    "- Multi-valued categorical (value of a pixel can be 0 ~ 255)\n",
    "- Real-valued\n",
    "\n",
    "\n",
    "Thus, we will implement two NB classifiers:\n",
    "- Gaussian NB model \n",
    "- Multinomial NB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No. of Samples:  (70000, 784)\n",
      "No. of Labels:  (70000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/hasan/anaconda/lib/python3.5/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "#print(mnist)\n",
    "\n",
    "X, y = mnist[\"data\"], mnist[\"target\"] \n",
    "\n",
    "\n",
    "print(\"\\nNo. of Samples: \", X.shape)\n",
    "print(\"No. of Labels: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display a Random Image\n",
    "\n",
    "To display a digit, we need to reshape an instance’s feature vector to a 28 x 28 array. \n",
    "\n",
    "For displaying we use Matplotlib’s imshow() function:."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_digit = X[34669]\n",
    "\n",
    "random_digit_image = random_digit.reshape(28, 28)\n",
    "\n",
    "plt.imshow(random_digit_image, cmap = plt.cm.binary, interpolation=\"nearest\")\n",
    "#plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train-Test Set\n",
    "\n",
    "The MNIST dataset is actually already split into a training set (the first 60,000 images) and a test set (the last 10,000 images):\n",
    "\n",
    "However, we need to shuffle the training set. This will guarantee that all cross-validation folds will be similar because we don’t want one fold to be missing some digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian NB: Model Selection\n",
    "\n",
    "We need to find the optimal value for the hyperparameter \"var_smoothing\".\n",
    "\n",
    "It represents the portion of the largest variance of all features that is added to variances for calculation stability. Its default vale is 1e-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (accuracy): 0.800583\n",
      "Optimal Hyperparameter Values:  {'var_smoothing': 0.1}\n",
      "CPU times: user 1min 31s, sys: 56.6 s, total: 2min 27s\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'var_smoothing': [0.001, 0.01, 0.1, 1.0]}\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb = GridSearchCV(gnb, param_grid, scoring='accuracy', cv=5)\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "params_optimal = gnb.best_params_\n",
    "\n",
    "print(\"Best Score (accuracy): %f\" % gnb.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Optimal Gaussian NB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 533 ms, sys: 189 ms, total: 723 ms\n",
      "Wall time: 722 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gaussianNB_clf = GaussianNB(**params_optimal)\n",
    "\n",
    "gaussianNB_clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Optimal Gaussian Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:  0.814\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 901    0    0    4    1   14   21    1   34    4]\n",
      " [   0 1097    1    5    0    0    5    0   27    0]\n",
      " [  15   43  778   30    9    4   57   15   76    5]\n",
      " [   6   41   25  810    0   18   14   12   42   42]\n",
      " [   3    9    3    0  644    2   21    1   19  280]\n",
      " [  19   35    6   96   19  568   24   13   61   51]\n",
      " [  12   26   10    1    9   19  868    0   12    1]\n",
      " [   1   41    7    4   22    0    1  840   25   87]\n",
      " [   6   72    8   39   11   18   12    4  736   68]\n",
      " [   7   19    2    8   40    2    0   15   18  898]]\n",
      "\n",
      "Test Precision = 0.814000\n",
      "\n",
      "Test Recall = 0.814000\n",
      "\n",
      "Test F1 Score = 0.814000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.92      0.92       980\n",
      "         1.0       0.79      0.97      0.87      1135\n",
      "         2.0       0.93      0.75      0.83      1032\n",
      "         3.0       0.81      0.80      0.81      1010\n",
      "         4.0       0.85      0.66      0.74       982\n",
      "         5.0       0.88      0.64      0.74       892\n",
      "         6.0       0.85      0.91      0.88       958\n",
      "         7.0       0.93      0.82      0.87      1028\n",
      "         8.0       0.70      0.76      0.73       974\n",
      "         9.0       0.63      0.89      0.73      1009\n",
      "\n",
      "   micro avg       0.81      0.81      0.81     10000\n",
      "   macro avg       0.83      0.81      0.81     10000\n",
      "weighted avg       0.83      0.81      0.81     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_predicted = gaussianNB_clf.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nAccuracy: \", accuracy_score_test)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "precision_test = precision_score(y_test, y_test_predicted, average='micro') \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, y_test_predicted, average='micro')\n",
    "print(\"\\nTest Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, y_test_predicted, average='micro')\n",
    "print(\"\\nTest F1 Score = %f\" % f1_test)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial NB: Model Selection\n",
    "\n",
    "We need to find the optimal value for the hyperparameter \"alpha\".\n",
    "\n",
    "It is the additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing). Its default vale is 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (accuracy): 0.824283\n",
      "Optimal Hyperparameter Values:  {'alpha': 0.1}\n",
      "CPU times: user 21.1 s, sys: 7.36 s, total: 28.5 s\n",
      "Wall time: 9.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'alpha': [0.1, 1.0, 2.0]}\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb = GridSearchCV(mnb, param_grid, scoring='accuracy', cv=5)\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "params_optimal = mnb.best_params_\n",
    "\n",
    "print(\"Best Score (accuracy): %f\" % mnb.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Optimal Multinomial NB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 433 ms, sys: 200 ms, total: 633 ms\n",
      "Wall time: 366 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "multinomialNB_clf = MultinomialNB(**params_optimal)\n",
    "\n",
    "multinomialNB_clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Optimal Multinomial Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:  0.8367\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 912    0    2    6    1    8   14    1   36    0]\n",
      " [   0 1061    5    9    0    2    6    0   51    1]\n",
      " [  15   11  859   23   10    3   33   11   66    1]\n",
      " [   4   11   34  851    1   21    7   14   40   27]\n",
      " [   2    2    6    0  732    0   25    1   38  176]\n",
      " [  23   11    6  107   18  590   17    6   78   36]\n",
      " [  16   13   17    1    7   25  861    0   18    0]\n",
      " [   1   21   11    5   18    0    1  861   40   70]\n",
      " [   6   26   13   54   14   27    8    9  777   40]\n",
      " [   6    7    3   10   66   10    0   17   27  863]]\n",
      "\n",
      "Test Precision = 0.836700\n",
      "\n",
      "Test Recall = 0.836700\n",
      "\n",
      "Test F1 Score = 0.836700\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.93      0.93       980\n",
      "         1.0       0.91      0.93      0.92      1135\n",
      "         2.0       0.90      0.83      0.86      1032\n",
      "         3.0       0.80      0.84      0.82      1010\n",
      "         4.0       0.84      0.75      0.79       982\n",
      "         5.0       0.86      0.66      0.75       892\n",
      "         6.0       0.89      0.90      0.89       958\n",
      "         7.0       0.94      0.84      0.88      1028\n",
      "         8.0       0.66      0.80      0.72       974\n",
      "         9.0       0.71      0.86      0.78      1009\n",
      "\n",
      "   micro avg       0.84      0.84      0.84     10000\n",
      "   macro avg       0.84      0.83      0.84     10000\n",
      "weighted avg       0.84      0.84      0.84     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_predicted_mnb = multinomialNB_clf.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted_mnb == y_test)\n",
    "print(\"\\nAccuracy: \", accuracy_score_test)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted_mnb))\n",
    "\n",
    "precision_test = precision_score(y_test, y_test_predicted_mnb, average='micro') \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, y_test_predicted_mnb, average='micro')\n",
    "print(\"\\nTest Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, y_test_predicted_mnb, average='micro')\n",
    "print(\"\\nTest F1 Score = %f\" % f1_test)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted_mnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial vs Gaussian Naive Bayes for Image Classification\n",
    "\n",
    "The multinomial Naive Bayes classifier performs better than the Gaussian variant. \n",
    "\n",
    "In fact, each sample can be thought as a feature vector derived from a dictionary of 764 symbols. The value can be the count of each occurrence, so a multinomial distribution can better fit the data, while a Gaussian is slightly more limited by its mean and variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier\n",
    "\n",
    "The model selection for the KNN classifier on the MNIST dataset takes awefully long time. \n",
    "\n",
    "Below we provide the commented out code that we used to find the optimal KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# param_grid_knn = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 4, 5]}]\n",
    "\n",
    "# knn = KNeighborsClassifier()\n",
    "\n",
    "# knn_cv = GridSearchCV(knn, param_grid_knn, cv=5, verbose=3, n_jobs=-1)\n",
    "\n",
    "# knn_cv.fit(X_train, y_train_5)\n",
    "\n",
    "# params_optimal_knn = knn_cv.best_params_\n",
    "\n",
    "# print(\"Best Score: %f\" % knn_cv.best_score_)\n",
    "# print(\"Optimal Hyperparameter Values: \", params_optimal_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Optimal KNN Classifier\n",
    "\n",
    "The optimal hyperparameters for the KNN model are:\n",
    "- weights = \"distance\"\n",
    "- n_neighbots = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 59s, sys: 1.07 s, total: 11min\n",
      "Wall time: 11min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=4, weights='distance')\n",
    "\n",
    "knn_clf.fit(X_train, y_train)\n",
    "\n",
    "y_test_predicted_knn = knn_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Optimal KNN Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:  0.9714\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 973    1    1    0    0    1    3    1    0    0]\n",
      " [   0 1132    2    0    0    0    1    0    0    0]\n",
      " [  10    5  995    2    1    0    0   16    3    0]\n",
      " [   0    1    3  974    1   14    1    7    4    5]\n",
      " [   1    5    0    0  950    0    4    3    0   19]\n",
      " [   4    0    0    9    2  862    7    1    3    4]\n",
      " [   4    2    0    0    3    3  946    0    0    0]\n",
      " [   0   17    4    0    3    0    0  994    0   10]\n",
      " [   5    2    4   14    5   11    4    4  920    5]\n",
      " [   3    4    2    7    9    4    1   10    1  968]]\n",
      "\n",
      "Test Precision = 0.971400\n",
      "\n",
      "Test Recall = 0.971400\n",
      "\n",
      "Test F1 Score = 0.971400\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98       980\n",
      "         1.0       0.97      1.00      0.98      1135\n",
      "         2.0       0.98      0.96      0.97      1032\n",
      "         3.0       0.97      0.96      0.97      1010\n",
      "         4.0       0.98      0.97      0.97       982\n",
      "         5.0       0.96      0.97      0.96       892\n",
      "         6.0       0.98      0.99      0.98       958\n",
      "         7.0       0.96      0.97      0.96      1028\n",
      "         8.0       0.99      0.94      0.97       974\n",
      "         9.0       0.96      0.96      0.96      1009\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted_knn == y_test)\n",
    "print(\"\\nAccuracy: \", accuracy_score_test)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted_knn))\n",
    "\n",
    "precision_test = precision_score(y_test, y_test_predicted_knn, average='micro') \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, y_test_predicted_knn, average='micro')\n",
    "print(\"\\nTest Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, y_test_predicted_knn, average='micro')\n",
    "print(\"\\nTest F1 Score = %f\" % f1_test)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "\n",
    "\n",
    "While the KNN clssifier accuracy is 97%, the NB classifier (multinomial) accuracy is only about 83%. \n",
    "\n",
    "The poor perfomance of the NB classifier is due to the inadequacy of the naive Bayes assumption in the image classfication domain, especially when pixels represent the feature values. It is indeed incorrect to assume that each and every pixel are independently generated, depending only on the label. \n",
    "\n",
    "Thus, while we can independently compute the likelihood of the values of a feature vector in text classfication, and take the product of the individual likelihood values to estimate the feature vector likelihood, this is indeed not the case in image classification.\n",
    "\n",
    "The words (mostly) in a document are representative micro-unit of the class of the document. Each word independetly bear the \"smell\" of the class.\n",
    "\n",
    "However, in image classification, when we use pixel values as the values of a feature vector, they no more are representative of the class. The pixel micro-units don't have enough \"information\" to individually relate to the class of the feature.\n",
    "\n",
    "However, we can use clusters of pixels (meaningful patches from an image) to define \"visual words\" and use them as components of the feature vector. By doing this we can extend the \"bag of words\" model to solve object classification in computer vision using NB algorithm.\n",
    "\n",
    "More detail on object recognition using visual \"bag of words\" model:\n",
    "http://vision.stanford.edu/teaching/cs231a_autumn1112/lecture/lecture15_bow_part-based_cs231a_marked.pdf\n",
    "\n",
    "\n",
    "# <font color=red> Key Lesson:</font>\n",
    "\n",
    "The success of the NB algorithm depends on the model the feature values. If the feature values (textual or visual words) independently relate to the class of the sample, then we can apply the NB assumption. And as long as we can apply the NB assumption meaningfully, the NB algorithm will perform well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
