{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Simplicity of Naive Bayes Classifiers: Benefit and Bane\n",
    "\n",
    "\n",
    "The goal of this notebook is to understand the **benefit and bane** of the Naive Bayes (NB) algorithm.\n",
    "\n",
    "More specifically, we will **<font color=red> investigate the limitation</font>** of the NB classifier.\n",
    "\n",
    "First, let's briefly discuss the benefit of the NB classifier.\n",
    "\n",
    "\n",
    "## Benefit of \"Simplicity\"\n",
    "\n",
    "The Naive Bayes algorithm is fueled by two \"simplicity\".\n",
    "    - Simple inductive bias: conditional independence\n",
    "    - Simple computation: statistical estimation of the prior and likelihood\n",
    "\n",
    "\n",
    "In the NB classifier, we make a strong assumption: **given the class, the features are conditionaly independent**. The model is called \"naive\" because we do not expect the features to be independent, even conditional on the class label.\n",
    "\n",
    "However, for solving text classification problems we see that although the naive Bayes assumption is an extremely strong assumption, the performance of the NB classifier is surprisingly good.\n",
    "\n",
    "The reason NB classifier works well in text classification problems is that the conditional independence is a reasonalble assumption in the context of text classification. \n",
    "\n",
    "As an example, consider classifying emails into spam and ham. Words like \"lottery\", \"prize\" and \"win\" are all likely indicators that the email might be spam. On the other hand words like \"Bayes\", \"Learning\", \"Classifier\" are good indicators of a ham email. Thus, we could model the **probability of occurrence for each of these words independently, given the respective class** and then use it to score the likelihood of a text.\n",
    "\n",
    "\n",
    "\n",
    "## Bane of \"Simplicity\"\n",
    "\n",
    "However, the conditional independence assumption might not hold in some domains. For example, in image classification problems if we use pixel values to model the feature vectors, then the NB assumption fails. The reason is that the feature values (pixels) are not conditionally independent. The **pixels are not independently generated, depending only on the label**. If we look at a black pixel at one of the corners, the other pixels around is is likely to be black.\n",
    "\n",
    "Thus, the NB assumption does not hold for the feature values (pixels) in an image classification problem.\n",
    "\n",
    "Because of its simplicity, Naive Bayes was a very popular choice for building spam filters during 80s and 90s. However, for classifying images it is not as effective. The main reason, as we identified, is the inadequacy of its simplicity! The inductive bias (conditional independence assumption) simply doesn't hold in many domains (e.g., image classification).\n",
    "\n",
    "It is due to this incorrect assumption about the \"world\", NB classifiers lost its popularity. Advanced models such as Artificial Neural Network was invented for image classification. \n",
    "\n",
    "\n",
    "In this notebook, we will use NB classifier to **solve an image classification problem**. We will compare the performance of the NB classifier with the K-Nearest Neighbor (KNN) memory-based model that we learned previously.\n",
    "\n",
    "\n",
    "## Datset\n",
    "\n",
    "We use the MNIST (Modified National Institute of Standards and Technology) dataset. It is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. \n",
    "\n",
    "Each image is labeled with the digit it represents. Each image is represented by 28 x 28 gray scale. There are 784 pixel values. Each value represents a pixel’s intensity, from 0 (white) to 255 (black). \n",
    "\n",
    "\n",
    "## NB Models for Investigation\n",
    "\n",
    "We model a feature vector (an image) as a 784 dimensional vector. Each dimension represents an individual pixel value. Since the pixels represent light intensity in the range of 0 ~ 255, we have two choices to model the feature values.\n",
    "\n",
    "- Multi-valued categorical (value of a pixel can be 0 ~ 255)\n",
    "- Real-valued\n",
    "\n",
    "\n",
    "Thus, we implement two NB classifiers:\n",
    "- Gaussian NB model \n",
    "- Multinomial NB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data, Create Data Matrix (X) & Target Vector (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['categories', 'DESCR', 'details', 'url', 'target', 'feature_names', 'data'])\n",
      "['5' '0' '4' ... '4' '5' '6']\n",
      "\n",
      "No. of Samples:  (70000, 784)\n",
      "No. of Labels:  (70000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784', cache=False)\n",
    "\n",
    "#print(\"DESCR: \\n\", mnist.DESCR)\n",
    "print(mnist.keys())\n",
    "print(mnist.target)\n",
    "\n",
    "\n",
    "X = mnist.data.astype('float32')\n",
    "y = mnist.target.astype('int64')\n",
    "\n",
    "\n",
    "print(\"\\nNo. of Samples: \", X.shape)\n",
    "print(\"No. of Labels: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display a Random Image\n",
    "\n",
    "To display a digit, we need to reshape an instance’s feature vector to a 28 x 28 array. \n",
    "\n",
    "For displaying we use Matplotlib’s imshow() function:."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njUu7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_digit = X[33528]\n",
    "\n",
    "random_digit_image = random_digit.reshape(28, 28)\n",
    "\n",
    "plt.imshow(random_digit_image, cmap = plt.cm.binary, interpolation=\"nearest\")\n",
    "#plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Saling\n",
    "\n",
    "To avoid big weights that deal with the pixel values from between [0, 255], we scale X down.\n",
    "\n",
    "A commonly used range is [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X /= 255.0\n",
    "\n",
    "X.min(), X.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test Dataset\n",
    "\n",
    "We use sklearn's train_test_split function to spilt the dataset into training and test subsets. The data is shuffled by default before splitting.\n",
    "\n",
    "This function splits arrays or matrices into **random** train and test subsets.\n",
    "\n",
    "For the **reproducibility of the results**, we need to use the same seed for the random number generator. The seed is set by the \"random_state\" parameter of the split function. \n",
    "\n",
    "However, in repeated experiments if we don't want to use the same train and test subsets, then we drop the \"random_state\" parameter from the funtion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian NB: Model Selection\n",
    "\n",
    "We need to find the optimal value for the hyperparameter \"var_smoothing\".\n",
    "\n",
    "It represents the portion of the largest variance of all features that is added to variances for calculation stability. Its default vale is 1e-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   15.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (accuracy): 0.804143\n",
      "Optimal Hyperparameter Values:  {'var_smoothing': 0.1}\n",
      "CPU times: user 675 ms, sys: 445 ms, total: 1.12 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'var_smoothing': [0.001, 0.01, 0.1, 1.0]}\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb = GridSearchCV(gnb, param_grid, scoring='accuracy', cv=10, verbose=1, n_jobs=-1)\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "params_optimal = gnb.best_params_\n",
    "\n",
    "print(\"Best Score (accuracy): %f\" % gnb.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Optimal Gaussian NB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 333 ms, sys: 98 ms, total: 431 ms\n",
      "Wall time: 429 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gaussianNB_clf = GaussianNB(**params_optimal)\n",
    "\n",
    "gaussianNB_clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Optimal Gaussian Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:  0.7939285714285714\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[1243    0    3    6    1   25   37    0   61   11]\n",
      " [   0 1537    2    6    2    1    9    0   21    2]\n",
      " [  18   62 1078   37   14    2  106   14  103    9]\n",
      " [   7   87   63 1066    1   32   16   21   76   66]\n",
      " [   2   19   10    0  887   10   21    3   29  369]\n",
      " [  31   46   12  150   30  760   34   12   92   64]\n",
      " [  16   40   14    0   13   20 1260    1   23    0]\n",
      " [   6   72    6    4   44    0    1 1149   37  139]\n",
      " [   5  167   15   60   15   28    8    2  973   95]\n",
      " [  11   45    5   11   60    2    0   30   35 1162]]\n",
      "\n",
      "Test Precision = 0.793929\n",
      "\n",
      "Test Recall = 0.793929\n",
      "\n",
      "Test F1 Score = 0.793929\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.91      1387\n",
      "           1       0.74      0.97      0.84      1580\n",
      "           2       0.89      0.75      0.81      1443\n",
      "           3       0.80      0.74      0.77      1435\n",
      "           4       0.83      0.66      0.73      1350\n",
      "           5       0.86      0.62      0.72      1231\n",
      "           6       0.84      0.91      0.88      1387\n",
      "           7       0.93      0.79      0.85      1458\n",
      "           8       0.67      0.71      0.69      1368\n",
      "           9       0.61      0.85      0.71      1361\n",
      "\n",
      "    accuracy                           0.79     14000\n",
      "   macro avg       0.81      0.79      0.79     14000\n",
      "weighted avg       0.81      0.79      0.79     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_predicted = gaussianNB_clf.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nAccuracy: \", accuracy_score_test)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "precision_test = precision_score(y_test, y_test_predicted, average='micro') \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, y_test_predicted, average='micro')\n",
    "print(\"\\nTest Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, y_test_predicted, average='micro')\n",
    "print(\"\\nTest F1 Score = %f\" % f1_test)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial NB: Model Selection\n",
    "\n",
    "We need to find the optimal value for the hyperparameter \"alpha\".\n",
    "\n",
    "It is the additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing). Its default vale is 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    6.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (accuracy): 0.827286\n",
      "Optimal Hyperparameter Values:  {'alpha': 0.1}\n",
      "CPU times: user 667 ms, sys: 383 ms, total: 1.05 s\n",
      "Wall time: 6.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {'alpha': [0.1, 1.0, 2.0]}\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb = GridSearchCV(mnb, param_grid, scoring='accuracy', cv=10, verbose=1, n_jobs=-1)\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "params_optimal = mnb.best_params_\n",
    "\n",
    "print(\"Best Score (accuracy): %f\" % mnb.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Optimal Multinomial NB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 404 ms, sys: 139 ms, total: 543 ms\n",
      "Wall time: 359 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "multinomialNB_clf = MultinomialNB(**params_optimal)\n",
    "\n",
    "multinomialNB_clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Optimal Multinomial Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:  0.8187857142857143\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[1260    0    7    7    2   19   23    0   69    0]\n",
      " [   0 1487    8   14    3    3    9    0   54    2]\n",
      " [  21   20 1181   33   12    0   71   12   88    5]\n",
      " [   3   27   80 1145    0   37    9   17   69   48]\n",
      " [   2    3    8    2 1008    6   25    3   56  237]\n",
      " [  33   11   17  155   29  794   30    3  119   40]\n",
      " [  19   24   24    0   13   25 1261    0   21    0]\n",
      " [   9   20   13    8   46    0    2 1169   69  122]\n",
      " [   4   83   17   85   23   39    8    3 1047   59]\n",
      " [   9   16    8   17   87    3    0   41   69 1111]]\n",
      "\n",
      "Test Precision = 0.818786\n",
      "\n",
      "Test Recall = 0.818786\n",
      "\n",
      "Test F1 Score = 0.818786\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92      1387\n",
      "           1       0.88      0.94      0.91      1580\n",
      "           2       0.87      0.82      0.84      1443\n",
      "           3       0.78      0.80      0.79      1435\n",
      "           4       0.82      0.75      0.78      1350\n",
      "           5       0.86      0.65      0.74      1231\n",
      "           6       0.88      0.91      0.89      1387\n",
      "           7       0.94      0.80      0.86      1458\n",
      "           8       0.63      0.77      0.69      1368\n",
      "           9       0.68      0.82      0.74      1361\n",
      "\n",
      "    accuracy                           0.82     14000\n",
      "   macro avg       0.83      0.82      0.82     14000\n",
      "weighted avg       0.83      0.82      0.82     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_predicted_mnb = multinomialNB_clf.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted_mnb == y_test)\n",
    "print(\"\\nAccuracy: \", accuracy_score_test)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted_mnb))\n",
    "\n",
    "precision_test = precision_score(y_test, y_test_predicted_mnb, average='micro') \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, y_test_predicted_mnb, average='micro')\n",
    "print(\"\\nTest Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, y_test_predicted_mnb, average='micro')\n",
    "print(\"\\nTest F1 Score = %f\" % f1_test)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted_mnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial vs Gaussian Naive Bayes for Image Classification\n",
    "\n",
    "The multinomial Naive Bayes classifier performs better than the Gaussian variant. \n",
    "\n",
    "In fact, each sample can be thought as a feature vector derived from a dictionary of 764 symbols. The value can be the count of each occurrence, so a multinomial distribution can better fit the data, while a Gaussian is slightly more limited by its mean and variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier\n",
    "\n",
    "The model selection for the KNN classifier on the MNIST dataset takes awefully long time. \n",
    "\n",
    "Below we provide the commented out code that we used to find the optimal KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# param_grid_knn = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 4, 5]}]\n",
    "\n",
    "# knn = KNeighborsClassifier()\n",
    "\n",
    "# knn_cv = GridSearchCV(knn, param_grid_knn, cv=5, verbose=3, n_jobs=-1)\n",
    "\n",
    "# knn_cv.fit(X_train, y_train_5)\n",
    "\n",
    "# params_optimal_knn = knn_cv.best_params_\n",
    "\n",
    "# print(\"Best Score: %f\" % knn_cv.best_score_)\n",
    "# print(\"Optimal Hyperparameter Values: \", params_optimal_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Optimal KNN Classifier\n",
    "\n",
    "The optimal hyperparameters for the KNN model are:\n",
    "- weights = \"distance\"\n",
    "- n_neighbots = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 43s, sys: 2.23 s, total: 14min 46s\n",
      "Wall time: 14min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=4, weights='distance')\n",
    "\n",
    "knn_clf.fit(X_train, y_train)\n",
    "\n",
    "y_test_predicted_knn = knn_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Optimal KNN Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:  0.9734285714285714\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[1374    1    1    1    0    0    9    0    1    0]\n",
      " [   0 1576    2    1    0    0    0    1    0    0]\n",
      " [   7    8 1397    3    2    0    1   18    6    1]\n",
      " [   0    2   17 1380    1   11    0    7   15    2]\n",
      " [   1    6    0    0 1312    0    0    4    1   26]\n",
      " [   1    2    1   10    1 1187   18    2    3    6]\n",
      " [   4    2    0    0    5    6 1369    0    1    0]\n",
      " [   0   16    5    0    1    0    0 1422    1   13]\n",
      " [   2   19    3   11    4   20    4    0 1287   18]\n",
      " [   1    0    1    8   11    2    1   12    1 1324]]\n",
      "\n",
      "Test Precision = 0.973429\n",
      "\n",
      "Test Recall = 0.973429\n",
      "\n",
      "Test F1 Score = 0.973429\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1387\n",
      "           1       0.97      1.00      0.98      1580\n",
      "           2       0.98      0.97      0.97      1443\n",
      "           3       0.98      0.96      0.97      1435\n",
      "           4       0.98      0.97      0.98      1350\n",
      "           5       0.97      0.96      0.97      1231\n",
      "           6       0.98      0.99      0.98      1387\n",
      "           7       0.97      0.98      0.97      1458\n",
      "           8       0.98      0.94      0.96      1368\n",
      "           9       0.95      0.97      0.96      1361\n",
      "\n",
      "    accuracy                           0.97     14000\n",
      "   macro avg       0.97      0.97      0.97     14000\n",
      "weighted avg       0.97      0.97      0.97     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted_knn == y_test)\n",
    "print(\"\\nAccuracy: \", accuracy_score_test)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted_knn))\n",
    "\n",
    "precision_test = precision_score(y_test, y_test_predicted_knn, average='micro') \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, y_test_predicted_knn, average='micro')\n",
    "print(\"\\nTest Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, y_test_predicted_knn, average='micro')\n",
    "print(\"\\nTest F1 Score = %f\" % f1_test)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "\n",
    "\n",
    "While the KNN clssifier accuracy is 97%, the NB classifier (multinomial) accuracy is only about 82%. \n",
    "\n",
    "The poor perfomance of the NB classifier is due to the inadequacy of the naive Bayes assumption in the image classfication domain, especially when pixels represent the feature values. It is indeed incorrect to assume that each and every pixel are independently generated, depending only on the label. \n",
    "\n",
    "Thus, while we can independently compute the likelihood of the values of a feature vector in text classfication, and take the product of the individual likelihood values to estimate the feature vector likelihood, this is indeed not the case in image classification.\n",
    "\n",
    "The words (mostly) in a document are representative micro-unit of the class of the document. Each word independetly bear the \"smell\" of the class.\n",
    "\n",
    "However, in image classification, when we use pixel values as the values of a feature vector, they no more are representative of the class. The pixel micro-units don't have enough \"information\" to individually relate to the class of the feature.\n",
    "\n",
    "However, we can use clusters of pixels (meaningful patches from an image) to define \"visual words\" and use them as components of the feature vector. By doing this we can extend the \"bag of words\" model to solve object classification in computer vision using NB algorithm.\n",
    "\n",
    "More detail on object recognition using visual \"bag of words\" model:\n",
    "http://vision.stanford.edu/teaching/cs231a_autumn1112/lecture/lecture15_bow_part-based_cs231a_marked.pdf\n",
    "\n",
    "\n",
    "# <font color=red> Key Lesson:</font>\n",
    "\n",
    "The success of the NB algorithm depends the feature values. If the feature values (textual or visual words) independently relate to the class of the sample, then we can apply the NB assumption. And as long as we can apply the NB assumption meaningfully, the NB algorithm will perform well."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
